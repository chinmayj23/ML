{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_lab8_J079.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chinmayj23/ML/blob/master/ML_lab8_J079.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR2Z7NRm7B_g"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import requests\n",
        "import cv2\n",
        "import PIL.Image\n",
        "import urllib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import plotly.graph_objects as go\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ4mV1qMdtQW"
      },
      "source": [
        "!mkdir /content/IndAfrTusk_train/ \n",
        "!mkdir /content/IndAfrTusk_valid/\n",
        "!mkdir /content/IndAfrTusk_test/"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbEbH4fFw2Tv"
      },
      "source": [
        "indian_elephant = requests.get(\"http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n02504013\")\n",
        "ind_elephant_soup = BeautifulSoup(indian_elephant.content, 'html.parser')\n",
        "ind_elephant_str = str(ind_elephant_soup)\n",
        "ind_ele_urls = ind_elephant_str.split('\\r\\n')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KRdJjbWx9I9"
      },
      "source": [
        "african_elephant = requests.get(\"http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n02504458\")\n",
        "afr_elephant_soup = BeautifulSoup(african_elephant.content, 'html.parser')\n",
        "afr_elephant_str = str(afr_elephant_soup)\n",
        "afr_ele_urls = afr_elephant_str.split('\\r\\n')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASdGFvtOw2Zb"
      },
      "source": [
        "tusker = requests.get(\"http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n01871265\")\n",
        "tusker_soup = BeautifulSoup(tusker.content, 'html.parser')\n",
        "tusker_str = str(tusker_soup)\n",
        "tusker_urls = tusker_str.split('\\r\\n')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe56fs3Zw2S5"
      },
      "source": [
        "img_rows, img_cols = 240, 240\n",
        "input_shape = (img_rows, img_cols, 3)\n",
        "num_train = 210\n",
        "num_valid = 70\n",
        "num_test = 70"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEsVhwKJw2Md"
      },
      "source": [
        "def url_to_image(url):\n",
        "\tresp = urllib.request.urlopen(url)\n",
        "\timage = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "\timage = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "\treturn image"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_1sI5dHw2F4",
        "outputId": "a7574c8c-24d3-4aa6-bba5-cd5bede7a60d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for progress in tqdm(range(num_train)):\n",
        "    if not ind_ele_urls[progress] == None:\n",
        "      try:\n",
        "        I = url_to_image(ind_ele_urls[progress])\n",
        "        if (len(I.shape))==3:\n",
        "          save_path = '/content/IndAfrTusk_train/Indian.'+str(progress)+'.jpg'\n",
        "          cv2.imwrite(save_path,I)\n",
        "      except:\n",
        "        None\n",
        "\n",
        "for progress in tqdm(range(num_train)):\n",
        "    if not afr_ele_urls[progress] == None:\n",
        "      try:\n",
        "        I = url_to_image(afr_ele_urls[progress])\n",
        "        if (len(I.shape))==3:\n",
        "          save_path = '/content/IndAfrTusk_train/African.'+str(progress)+'.jpg'\n",
        "          cv2.imwrite(save_path,I)\n",
        "      except:\n",
        "        None\n",
        "\n",
        "for progress in tqdm(range(num_train)):\n",
        "    if not tusker_urls[progress] == None:\n",
        "      try:\n",
        "        I = url_to_image(tusker_urls[progress])\n",
        "        if (len(I.shape))==3:\n",
        "          save_path = '/content/IndAfrTusk_train/Tusker.'+str(progress)+'.jpg'\n",
        "          cv2.imwrite(save_path,I)\n",
        "      except:\n",
        "        None"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 210/210 [03:31<00:00,  1.01s/it]\n",
            "100%|██████████| 210/210 [02:34<00:00,  1.36it/s]\n",
            "100%|██████████| 210/210 [04:06<00:00,  1.18s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOektAKAyFA1",
        "outputId": "eecf2078-1e82-46c7-89f0-1e873a4b0256",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for progress in tqdm(range(num_valid)):\n",
        "    if not ind_ele_urls[progress] == None:\n",
        "      try:\n",
        "        I = url_to_image(ind_ele_urls[progress])\n",
        "        if (len(I.shape))==3:\n",
        "          save_path = '/content/IndAfrTusk_valid/Indian.'+str(progress)+'.jpg'\n",
        "          cv2.imwrite(save_path,I)\n",
        "      except:\n",
        "        None\n",
        "\n",
        "for progress in tqdm(range(num_valid)):\n",
        "    if not afr_ele_urls[progress] == None:\n",
        "      try:\n",
        "        I = url_to_image(afr_ele_urls[progress])\n",
        "        if (len(I.shape))==3:\n",
        "          save_path = '/content/IndAfrTusk_valid/African.'+str(progress)+'.jpg'\n",
        "          cv2.imwrite(save_path,I)\n",
        "      except:\n",
        "        None\n",
        "\n",
        "for progress in tqdm(range(num_valid)):\n",
        "    if not tusker_urls[progress] == None:\n",
        "      try:\n",
        "        I = url_to_image(tusker_urls[progress])\n",
        "        if (len(I.shape))==3:\n",
        "          save_path = '/content/IndAfrTusk_valid/Tusker.'+str(progress)+'.jpg'\n",
        "          cv2.imwrite(save_path,I)\n",
        "      except:\n",
        "        None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 70/70 [01:29<00:00,  1.28s/it]\n",
            "100%|██████████| 70/70 [00:15<00:00,  4.47it/s]\n",
            "100%|██████████| 70/70 [01:38<00:00,  1.40s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaMvb_9YA-Wp",
        "outputId": "cb9b472a-c5b8-4c5a-d183-f621de4d6f43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for progress in tqdm(range(num_test)):\n",
        "    if not ind_ele_urls[progress] == None:\n",
        "      try:\n",
        "        I = url_to_image(ind_ele_urls[progress])\n",
        "        if (len(I.shape))==3:\n",
        "          save_path = '/content/IndAfrTusk_test/Indian.'+str(progress)+'.jpg'\n",
        "          cv2.imwrite(save_path,I)\n",
        "      except:\n",
        "        None\n",
        "        \n",
        "\n",
        "for progress in tqdm(range(num_test)):\n",
        "    if not afr_ele_urls[progress] == None:\n",
        "      try:\n",
        "        I = url_to_image(afr_ele_urls[progress])\n",
        "        if (len(I.shape))==3:\n",
        "          save_path = '/content/IndAfrTusk_test/African.'+str(progress)+'.jpg'\n",
        "          cv2.imwrite(save_path,I)\n",
        "      except:\n",
        "        None\n",
        "\n",
        "for progress in tqdm(range(num_test)):\n",
        "    if not tusker_urls[progress] == None:\n",
        "      try:\n",
        "        I = url_to_image(tusker_urls[progress])\n",
        "        if (len(I.shape))==3:\n",
        "          save_path = '/content/IndAfrTusk_test/Tusker.'+str(progress)+'.jpg'\n",
        "          cv2.imwrite(save_path,I)\n",
        "      except:\n",
        "        None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 70/70 [00:56<00:00,  1.23it/s]\n",
            "100%|██████████| 70/70 [00:15<00:00,  4.45it/s]\n",
            "100%|██████████| 70/70 [01:43<00:00,  1.48s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7lN5D0MyFM0"
      },
      "source": [
        "original_train = '/content/IndAfrTusk_train'\n",
        "original_valid = '/content/IndAfrTusk_valid'\n",
        "original_test = '/content/IndAfrTusk_test'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykRSN6T4yFXU"
      },
      "source": [
        "filenames = os.listdir(original_train)\n",
        "cat_train = []\n",
        "for filename in filenames:\n",
        "    category = filename.split('.')[0]\n",
        "    if category == 'Indian':\n",
        "        cat_train.append('0')\n",
        "    elif category == 'African':\n",
        "        cat_train.append('1')\n",
        "    else:\n",
        "        cat_train.append('2')\n",
        "\n",
        " \n",
        "filenames = os.listdir(original_valid)\n",
        "cat_valid = []\n",
        "for filename in filenames:\n",
        "    category = filename.split('.')[0]\n",
        "    if category == 'Indian':\n",
        "        cat_valid.append('0')\n",
        "    elif category == 'African':\n",
        "        cat_valid.append('1')\n",
        "    else:\n",
        "        cat_valid.append('2')\n",
        "\n",
        "\n",
        "filenames = os.listdir(original_test)\n",
        "cat_test = []\n",
        "for filename in filenames:\n",
        "    category = filename.split('.')[0]\n",
        "    if category == 'Indian':\n",
        "        cat_test.append('0')\n",
        "    elif category == 'African':\n",
        "        cat_test.append('1')\n",
        "    else:\n",
        "        cat_test.append('2')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZvW5w-jyFfO"
      },
      "source": [
        "train = pd.DataFrame({'File':os.listdir(original_train),'Label':cat_train})\n",
        "train.to_csv('IndAfrTusk_train.csv',index=False)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvZgSU3DyfXO"
      },
      "source": [
        "valid = pd.DataFrame({'File':os.listdir(original_valid),'Label':cat_valid})\n",
        "valid.to_csv('IndAfrTusk_valid.csv',index=False)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lwaMp8Lyfd9"
      },
      "source": [
        "test = pd.DataFrame({'File':os.listdir(original_test),'Label':cat_test})\n",
        "test.to_csv('IndAfrTusk_test.csv',index=False)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEEQd4jml1-h",
        "outputId": "eea11a41-7b89-4166-b086-3031aa6c7c75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test.shape"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlUI0qGayfco",
        "outputId": "9e724b6b-b804-4ed5-848f-fb4b0a4dd357",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(404, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsjCpd3WlhD3",
        "outputId": "2bcca542-12b5-4347-db4c-2501f7be72d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Tusker.81.jpg</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>African.23.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>African.70.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Indian.152.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>African.76.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             File Label\n",
              "0   Tusker.81.jpg     2\n",
              "1  African.23.jpg     1\n",
              "2  African.70.jpg     1\n",
              "3  Indian.152.jpg     0\n",
              "4  African.76.jpg     1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElD2kNi_ljp-",
        "outputId": "68c00c01-86ef-443c-9024-551464210f09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        }
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [File, Label]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd_-R9PeyfWH"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1/255.0,height_shift_range=0.2,width_shift_range=0.2,zoom_range=0.1,vertical_flip=True,horizontal_flip=True,rotation_range=90,shear_range=0.1,fill_mode='nearest')\n",
        "valid_datagen = ImageDataGenerator(rescale=1/255.0,height_shift_range=0.2,width_shift_range=0.2,zoom_range=0.1,vertical_flip=True,horizontal_flip=True,rotation_range=90,shear_range=0.1,fill_mode='nearest')\n",
        "test_datagen = ImageDataGenerator(rescale=1/255.0)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrDNK8PMfcf8",
        "outputId": "2f1ebf1a-9792-435f-b7d6-d4216e1290bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_aug = train_datagen.flow_from_dataframe(dataframe=train,directory=original_train,x_col='File',y_col='Label',target_size=(img_rows, img_cols),class_mode='categorical',shuffle=True,seed=0,color_mode='rgb')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 404 validated image filenames belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1heFnEC5yFuq"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(512, kernel_size=(img_rows, img_cols), padding='valid', activation='relu',input_shape=input_shape))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=512, activation='relu'))\n",
        "model.add(Dense(units=512, activation='relu'))\n",
        "model.add(Dense(units=512, activation='relu'))\n",
        "model.add(Dense(units=512, activation='relu'))\n",
        "model.add(Dense(units=3, activation='softmax'))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zTi86snyyu-"
      },
      "source": [
        "filepath = \"weights-best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'val_accuracy', verbose=1, patience=3, save_best_only=True, mode='max')\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3, min_delta=0.001, baseline=0.9)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', mode='min', patience=3, factor=0.01, min_lr=0.00001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBoZnBl4y3xj",
        "outputId": "6099ffce-68df-4866-9b78-4c2e5a470bb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "history = model.fit(train_aug, epochs=10, callbacks=[checkpoint,es,reduce_lr])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.1528 - accuracy: 0.3911WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
            "13/13 [==============================] - 56s 4s/step - loss: 1.1528 - accuracy: 0.3911\n",
            "Epoch 2/10\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.1001 - accuracy: 0.3985WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
            "13/13 [==============================] - 54s 4s/step - loss: 1.1001 - accuracy: 0.3985\n",
            "Epoch 3/10\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.1097 - accuracy: 0.3614WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
            "13/13 [==============================] - 54s 4s/step - loss: 1.1097 - accuracy: 0.3614\n",
            "Epoch 4/10\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.0931 - accuracy: 0.3614WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
            "13/13 [==============================] - 54s 4s/step - loss: 1.0931 - accuracy: 0.3614\n",
            "Epoch 5/10\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.0941 - accuracy: 0.3837WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
            "13/13 [==============================] - 57s 4s/step - loss: 1.0941 - accuracy: 0.3837\n",
            "Epoch 6/10\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.0878 - accuracy: 0.3985WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
            "13/13 [==============================] - 54s 4s/step - loss: 1.0878 - accuracy: 0.3985\n",
            "Epoch 7/10\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.0846 - accuracy: 0.3787WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
            "13/13 [==============================] - 55s 4s/step - loss: 1.0846 - accuracy: 0.3787\n",
            "Epoch 8/10\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.0853 - accuracy: 0.3762WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
            "13/13 [==============================] - 56s 4s/step - loss: 1.0853 - accuracy: 0.3762\n",
            "Epoch 9/10\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.0922 - accuracy: 0.3738WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
            "13/13 [==============================] - 56s 4s/step - loss: 1.0922 - accuracy: 0.3738\n",
            "Epoch 10/10\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.0815 - accuracy: 0.4010WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n",
            "13/13 [==============================] - 54s 4s/step - loss: 1.0815 - accuracy: 0.4010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2AtBDycy3wv",
        "outputId": "4668f4a1-bd23-41f2-b146-573ee8bc18df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.evaluate(train_aug)#since test dataset was empty"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13/13 [==============================] - 8s 619ms/step - loss: 1.0834 - accuracy: 0.3886\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0834465026855469, 0.38861384987831116]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    }
  ]
}